{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94327,"sourceType":"datasetVersion","datasetId":50445}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets seqeval accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:51:03.081662Z","iopub.execute_input":"2025-10-14T20:51:03.082044Z","iopub.status.idle":"2025-10-14T20:52:25.475494Z","shell.execute_reply.started":"2025-10-14T20:51:03.082013Z","shell.execute_reply":"2025-10-14T20:52:25.474583Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================================================\n# 1️⃣ Imports\n# =========================================================\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForTokenClassification\n)\nfrom seqeval.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:25.477236Z","iopub.execute_input":"2025-10-14T20:52:25.477623Z","iopub.status.idle":"2025-10-14T20:52:55.462875Z","shell.execute_reply.started":"2025-10-14T20:52:25.477599Z","shell.execute_reply":"2025-10-14T20:52:55.462305Z"}},"outputs":[{"name":"stderr","text":"2025-10-14 20:52:41.776565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760475162.022025      90 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760475162.091100      90 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:55.463592Z","iopub.execute_input":"2025-10-14T20:52:55.464095Z","iopub.status.idle":"2025-10-14T20:52:55.467633Z","shell.execute_reply.started":"2025-10-14T20:52:55.464076Z","shell.execute_reply":"2025-10-14T20:52:55.466935Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Enable logging for debugging\ntransformers.utils.logging.set_verbosity_info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:55.468616Z","iopub.execute_input":"2025-10-14T20:52:55.468907Z","iopub.status.idle":"2025-10-14T20:52:55.492531Z","shell.execute_reply.started":"2025-10-14T20:52:55.468884Z","shell.execute_reply":"2025-10-14T20:52:55.491842Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =========================================================\n# 2️⃣ Setup (GPU / CPU)\n# =========================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:55.494015Z","iopub.execute_input":"2025-10-14T20:52:55.494560Z","iopub.status.idle":"2025-10-14T20:52:55.507236Z","shell.execute_reply.started":"2025-10-14T20:52:55.494542Z","shell.execute_reply":"2025-10-14T20:52:55.506727Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# 3️⃣ Load dataset from Kaggle input path\n# =========================================================\n# 📦 Dataset: https://www.kaggle.com/datasets/alaakhaled/conll003-englishversion\ndata_path = \"/kaggle/input/conll003-englishversion/\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:55.507809Z","iopub.execute_input":"2025-10-14T20:52:55.508028Z","iopub.status.idle":"2025-10-14T20:52:55.521322Z","shell.execute_reply.started":"2025-10-14T20:52:55.508003Z","shell.execute_reply":"2025-10-14T20:52:55.520660Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(data_path, \"train.txt\"), sep=\" \", names=[\"word\", \"pos\", \"chunk\", \"ner\"], skip_blank_lines=False)\nvalid_df = pd.read_csv(os.path.join(data_path, \"valid.txt\"), sep=\" \", names=[\"word\", \"pos\", \"chunk\", \"ner\"], skip_blank_lines=False)\ntest_df  = pd.read_csv(os.path.join(data_path, \"test.txt\"),  sep=\" \", names=[\"word\", \"pos\", \"chunk\", \"ner\"], skip_blank_lines=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:52:55.522409Z","iopub.execute_input":"2025-10-14T20:52:55.522845Z","iopub.status.idle":"2025-10-14T20:52:55.750113Z","shell.execute_reply.started":"2025-10-14T20:52:55.522827Z","shell.execute_reply":"2025-10-14T20:52:55.749363Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"Train shape:\", train_df.shape)\nprint(\"Validation shape:\", valid_df.shape)\nprint(\"Test shape:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:53:16.417319Z","iopub.execute_input":"2025-10-14T20:53:16.417931Z","iopub.status.idle":"2025-10-14T20:53:16.422909Z","shell.execute_reply.started":"2025-10-14T20:53:16.417903Z","shell.execute_reply":"2025-10-14T20:53:16.422004Z"}},"outputs":[{"name":"stdout","text":"Train shape: (219554, 4)\nValidation shape: (55044, 4)\nTest shape: (50350, 4)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# =========================================================\n# 5️⃣ Convert CoNLL format → sentence + label lists\n# =========================================================\ndef read_conll(df):\n    sentences, labels = [], []\n    sent, labs = [], []\n    for word, tag in zip(df[\"word\"], df[\"ner\"]):\n        if str(word) == \"nan\":\n            if sent:\n                sentences.append(sent)\n                labels.append(labs)\n                sent, labs = [], []\n        else:\n            sent.append(word)\n            labs.append(tag)\n    if sent:\n        sentences.append(sent)\n        labels.append(labs)\n    return sentences, labels\n\ntrain_sentences, train_labels = read_conll(train_df)\nval_sentences, val_labels = read_conll(valid_df)\ntest_sentences, test_labels = read_conll(test_df)\n\nprint(\"\\nExample sentence:\")\nprint(train_sentences[1])\nprint(train_labels[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:53:37.295342Z","iopub.execute_input":"2025-10-14T20:53:37.295990Z","iopub.status.idle":"2025-10-14T20:53:37.388365Z","shell.execute_reply.started":"2025-10-14T20:53:37.295965Z","shell.execute_reply":"2025-10-14T20:53:37.387767Z"}},"outputs":[{"name":"stdout","text":"\nExample sentence:\n['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Clean NaN or invalid tags\ndef clean_labels(labels):\n    return [[t if isinstance(t, str) and t != 'nan' else 'O' for t in seq] for seq in labels]\n\ntrain_labels = clean_labels(train_labels)\nval_labels = clean_labels(val_labels)\ntest_labels = clean_labels(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:53:44.968331Z","iopub.execute_input":"2025-10-14T20:53:44.968883Z","iopub.status.idle":"2025-10-14T20:53:44.999345Z","shell.execute_reply.started":"2025-10-14T20:53:44.968859Z","shell.execute_reply":"2025-10-14T20:53:44.998608Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# =========================================================\n# 6️⃣ Map NER tags to numeric IDs\n# =========================================================\n# Identifies all NER tags and maps them to numeric IDs\nunique_tags = sorted(set(tag for seq in train_labels for tag in seq))\nprint(\"\\nUnique NER Tags:\", unique_tags)\n\ntag2id = {tag: i for i, tag in enumerate(unique_tags)}\nid2tag = {i: tag for tag, i in tag2id.items()}\nnum_labels = len(unique_tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:53:51.442085Z","iopub.execute_input":"2025-10-14T20:53:51.442359Z","iopub.status.idle":"2025-10-14T20:53:51.457718Z","shell.execute_reply.started":"2025-10-14T20:53:51.442340Z","shell.execute_reply":"2025-10-14T20:53:51.456991Z"}},"outputs":[{"name":"stdout","text":"\nUnique NER Tags: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# =========================================================\n# 7️⃣ Convert into Hugging Face Datasets\n# =========================================================\ntrain_dataset = Dataset.from_dict({\n    \"tokens\": train_sentences,\n    \"ner_tags\": [[tag2id[t] for t in seq] for seq in train_labels]\n})\nval_dataset = Dataset.from_dict({\n    \"tokens\": val_sentences,\n    \"ner_tags\": [[tag2id[t] for t in seq] for seq in val_labels]\n})\ntest_dataset = Dataset.from_dict({\n    \"tokens\": test_sentences,\n    \"ner_tags\": [[tag2id[t] for t in seq] for seq in test_labels]\n})\n\ndatasets = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\nprint(\"\\nDataset preview:\")\nprint(datasets)\n\n# Check dataset sizes\nprint(f\"Train: {len(datasets['train'])}, Validation: {len(datasets['validation'])}, Test: {len(datasets['test'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:53:59.624967Z","iopub.execute_input":"2025-10-14T20:53:59.625237Z","iopub.status.idle":"2025-10-14T20:53:59.785974Z","shell.execute_reply.started":"2025-10-14T20:53:59.625220Z","shell.execute_reply":"2025-10-14T20:53:59.785336Z"}},"outputs":[{"name":"stdout","text":"\nDataset preview:\nDatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 14991\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 3470\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 3690\n    })\n})\nTrain: 14991, Validation: 3470, Test: 3690\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# =========================================================\n# 8️⃣ Load tokenizer and model\n# =========================================================\nMODEL_NAME = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:54:20.335608Z","iopub.execute_input":"2025-10-14T20:54:20.335895Z","iopub.status.idle":"2025-10-14T20:54:22.205032Z","shell.execute_reply.started":"2025-10-14T20:54:20.335875Z","shell.execute_reply":"2025-10-14T20:54:22.204281Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfad865b50d04f2aa5aa9968d58a0295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc5ea0ecee54779825802ea29841291"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.53.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6deff10c90c4933b438e1805f251f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02aad2398b1740919fd6538673d3f06f"}},"metadata":{}},{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.53.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# =========================================================\n# 9️⃣ Tokenization & label alignment\n# =========================================================\nmax_length = 128\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=max_length,\n    )\n\n    all_labels = examples[\"ner_tags\"]\n    aligned_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(labels[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        aligned_labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = aligned_labels\n    return tokenized_inputs\n\ntokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:54:46.257498Z","iopub.execute_input":"2025-10-14T20:54:46.258316Z","iopub.status.idle":"2025-10-14T20:54:48.904799Z","shell.execute_reply.started":"2025-10-14T20:54:46.258285Z","shell.execute_reply":"2025-10-14T20:54:48.904109Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14991 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5b6f5cb60349978b5055fe00b41331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76b32f3fe96422d8f32fd5bfca67cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3690 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bdbb15b50ca4983880b2addff0b481b"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# =========================================================\n# 🔟 Data collator for dynamic padding\n# =========================================================\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:54:56.774770Z","iopub.execute_input":"2025-10-14T20:54:56.775510Z","iopub.status.idle":"2025-10-14T20:54:56.780250Z","shell.execute_reply.started":"2025-10-14T20:54:56.775475Z","shell.execute_reply":"2025-10-14T20:54:56.779486Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# =========================================================\n# 1️⃣1️⃣ Load model\n# =========================================================\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=num_labels,\n)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:55:04.432783Z","iopub.execute_input":"2025-10-14T20:55:04.433065Z","iopub.status.idle":"2025-10-14T20:55:07.879302Z","shell.execute_reply.started":"2025-10-14T20:55:04.433045Z","shell.execute_reply":"2025-10-14T20:55:07.878554Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.53.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1863a2ecab4033a9b2fc493bcd46ae"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\nA pretrained model of type `BertForTokenClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\nIf you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=9, bias=True)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# =========================================================\n# 1️⃣2️⃣ Define metrics\n# =========================================================\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=-1)\n    batch_size, seq_len = preds.shape\n    preds_list, labels_list = [], []\n\n    for i in range(batch_size):\n        pred_tags = []\n        true_tags = []\n        for j in range(seq_len):\n            if label_ids[i, j] == -100:\n                continue\n            pred_tags.append(id2tag[preds[i, j]])\n            true_tags.append(id2tag[label_ids[i, j]])\n        preds_list.append(pred_tags)\n        labels_list.append(true_tags)\n\n    return preds_list, labels_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:55:22.943207Z","iopub.execute_input":"2025-10-14T20:55:22.943816Z","iopub.status.idle":"2025-10-14T20:55:22.949202Z","shell.execute_reply.started":"2025-10-14T20:55:22.943791Z","shell.execute_reply":"2025-10-14T20:55:22.948420Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, label_ids = eval_pred\n    preds_list, labels_list = align_predictions(logits, label_ids)\n    return {\n        \"precision\": precision_score(labels_list, preds_list),\n        \"recall\": recall_score(labels_list, preds_list),\n        \"f1\": f1_score(labels_list, preds_list),\n        \"accuracy\": accuracy_score(labels_list, preds_list),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:55:29.614647Z","iopub.execute_input":"2025-10-14T20:55:29.614936Z","iopub.status.idle":"2025-10-14T20:55:29.619713Z","shell.execute_reply.started":"2025-10-14T20:55:29.614914Z","shell.execute_reply":"2025-10-14T20:55:29.618791Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# =========================================================\n# 1️⃣3️⃣ Training configuration\n# =========================================================\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-ner-conll2003\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=True,  # Mixed precision for faster GPU training\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    report_to=\"none\",  # Avoid W&B hangs\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:55:44.116380Z","iopub.execute_input":"2025-10-14T20:55:44.116703Z","iopub.status.idle":"2025-10-14T20:55:44.158456Z","shell.execute_reply.started":"2025-10-14T20:55:44.116683Z","shell.execute_reply":"2025-10-14T20:55:44.157795Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# =========================================================\n# 1️⃣4️⃣ Trainer setup\n# =========================================================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"] if len(datasets[\"validation\"]) > 0 else tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:55:58.792655Z","iopub.execute_input":"2025-10-14T20:55:58.793332Z","iopub.status.idle":"2025-10-14T20:55:58.815193Z","shell.execute_reply.started":"2025-10-14T20:55:58.793307Z","shell.execute_reply":"2025-10-14T20:55:58.814453Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_90/3108470709.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# =========================================================\n# 1️⃣5️⃣ Train\n# =========================================================\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T20:56:08.224223Z","iopub.execute_input":"2025-10-14T20:56:08.224991Z","iopub.status.idle":"2025-10-14T21:06:00.192751Z","shell.execute_reply.started":"2025-10-14T20:56:08.224954Z","shell.execute_reply":"2025-10-14T21:06:00.191999Z"}},"outputs":[{"name":"stderr","text":"The following columns in the Training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 14,991\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Training with DataParallel so batch size has been adjusted to: 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2,811\n  Number of trainable parameters = 107,726,601\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2811' max='2811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2811/2811 09:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.052700</td>\n      <td>0.045772</td>\n      <td>0.937699</td>\n      <td>0.943228</td>\n      <td>0.940455</td>\n      <td>0.989743</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.031900</td>\n      <td>0.067903</td>\n      <td>0.936075</td>\n      <td>0.947271</td>\n      <td>0.941639</td>\n      <td>0.989999</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.021300</td>\n      <td>0.099579</td>\n      <td>0.945309</td>\n      <td>0.952156</td>\n      <td>0.948720</td>\n      <td>0.990922</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 3470\n  Batch size = 16\nSaving model checkpoint to ./bert-ner-conll2003/checkpoint-937\nConfiguration saved in ./bert-ner-conll2003/checkpoint-937/config.json\nModel weights saved in ./bert-ner-conll2003/checkpoint-937/model.safetensors\ntokenizer config file saved in ./bert-ner-conll2003/checkpoint-937/tokenizer_config.json\nSpecial tokens file saved in ./bert-ner-conll2003/checkpoint-937/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThe following columns in the Evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 3470\n  Batch size = 16\nSaving model checkpoint to ./bert-ner-conll2003/checkpoint-1874\nConfiguration saved in ./bert-ner-conll2003/checkpoint-1874/config.json\nModel weights saved in ./bert-ner-conll2003/checkpoint-1874/model.safetensors\ntokenizer config file saved in ./bert-ner-conll2003/checkpoint-1874/tokenizer_config.json\nSpecial tokens file saved in ./bert-ner-conll2003/checkpoint-1874/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThe following columns in the Evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 3470\n  Batch size = 16\nSaving model checkpoint to ./bert-ner-conll2003/checkpoint-2811\nConfiguration saved in ./bert-ner-conll2003/checkpoint-2811/config.json\nModel weights saved in ./bert-ner-conll2003/checkpoint-2811/model.safetensors\ntokenizer config file saved in ./bert-ner-conll2003/checkpoint-2811/tokenizer_config.json\nSpecial tokens file saved in ./bert-ner-conll2003/checkpoint-2811/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./bert-ner-conll2003/checkpoint-2811 (score: 0.9487201007133864).\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2811, training_loss=0.052363134382543766, metrics={'train_runtime': 591.5108, 'train_samples_per_second': 76.031, 'train_steps_per_second': 4.752, 'total_flos': 1101398273750682.0, 'train_loss': 0.052363134382543766, 'epoch': 3.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# 1️⃣5️⃣ Evaluate\nprint(\"Validation results:\")\nval_metrics = trainer.evaluate(tokenized_datasets[\"validation\"])\nprint(val_metrics)\n\nprint(\"\\nTest results:\")\ntest_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\nprint(test_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T21:08:22.296931Z","iopub.execute_input":"2025-10-14T21:08:22.297430Z","iopub.status.idle":"2025-10-14T21:08:52.647041Z","shell.execute_reply.started":"2025-10-14T21:08:22.297398Z","shell.execute_reply":"2025-10-14T21:08:52.646201Z"}},"outputs":[{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 3470\n  Batch size = 16\n","output_type":"stream"},{"name":"stdout","text":"Validation results:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 3690\n  Batch size = 16\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.09957929700613022, 'eval_precision': 0.9453085800301053, 'eval_recall': 0.9521563342318059, 'eval_f1': 0.9487201007133864, 'eval_accuracy': 0.9909223091130586, 'eval_runtime': 14.9391, 'eval_samples_per_second': 232.277, 'eval_steps_per_second': 14.526, 'epoch': 3.0}\n\nTest results:\n{'eval_loss': 0.264486700296402, 'eval_precision': 0.9021758050478678, 'eval_recall': 0.9183203401842664, 'eval_f1': 0.9101764860830626, 'eval_accuracy': 0.9816525671260737, 'eval_runtime': 15.3934, 'eval_samples_per_second': 239.714, 'eval_steps_per_second': 15.006, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# 1️⃣6️⃣ Detailed classification report\npredictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\npreds_list, labels_list = align_predictions(predictions, labels)\nprint(\"\\nClassification report (test):\")\nprint(classification_report(labels_list, preds_list, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T21:08:52.648199Z","iopub.execute_input":"2025-10-14T21:08:52.648430Z","iopub.status.idle":"2025-10-14T21:09:08.512024Z","shell.execute_reply.started":"2025-10-14T21:08:52.648413Z","shell.execute_reply":"2025-10-14T21:09:08.511406Z"}},"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 3690\n  Batch size = 16\n","output_type":"stream"},{"name":"stdout","text":"\nClassification report (test):\n              precision    recall  f1-score   support\n\n         LOC     0.9322    0.9238    0.9279      1666\n        MISC     0.7843    0.8234    0.8033       702\n         ORG     0.8678    0.9127    0.8897      1661\n         PER     0.9627    0.9598    0.9612      1615\n\n   micro avg     0.9022    0.9183    0.9102      5644\n   macro avg     0.8867    0.9049    0.8955      5644\nweighted avg     0.9036    0.9183    0.9107      5644\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# 1️⃣7️⃣ Save model\nmodel.save_pretrained(\"./bert-ner-conll2003\")\ntokenizer.save_pretrained(\"./bert-ner-conll2003\")\nprint(\"✅ Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T21:09:55.648809Z","iopub.execute_input":"2025-10-14T21:09:55.649353Z","iopub.status.idle":"2025-10-14T21:09:56.543224Z","shell.execute_reply.started":"2025-10-14T21:09:55.649331Z","shell.execute_reply":"2025-10-14T21:09:56.542677Z"}},"outputs":[{"name":"stderr","text":"Configuration saved in ./bert-ner-conll2003/config.json\nModel weights saved in ./bert-ner-conll2003/model.safetensors\ntokenizer config file saved in ./bert-ner-conll2003/tokenizer_config.json\nSpecial tokens file saved in ./bert-ner-conll2003/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved successfully!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# 1️⃣8️⃣ Inference on a sample sentence\nx = \"The amazing company Google is about to layoff 1000 people\"\n\nsample_sentence = x.split()\nencoding = tokenizer(sample_sentence, is_split_into_words=True, return_tensors=\"pt\")\nencoding = {k: v.to(device) for k, v in encoding.items()}\n\nwith torch.no_grad():\n    outputs = model(**encoding)\npreds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n\npredicted_tags = [id2tag[p] for p in preds[1:len(sample_sentence)+1]]  # skip CLS, SEP\nprint(\"\\nSample sentence prediction:\")\nfor word, tag in zip(sample_sentence, predicted_tags):\n    print(f\"{word:<12} -> {tag}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T21:12:25.191379Z","iopub.execute_input":"2025-10-14T21:12:25.192043Z","iopub.status.idle":"2025-10-14T21:12:25.213375Z","shell.execute_reply.started":"2025-10-14T21:12:25.192013Z","shell.execute_reply":"2025-10-14T21:12:25.212459Z"}},"outputs":[{"name":"stdout","text":"\nSample sentence prediction:\nThe          -> O\namazing      -> O\ncompany      -> O\nGoogle       -> B-ORG\nis           -> O\nabout        -> O\nto           -> O\nlayoff       -> O\n1000         -> O\npeople       -> O\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!zip -r /kaggle/working/bert-ner-conll2003.zip /kaggle/working/bert-ner-conll2003","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T21:17:52.545217Z","iopub.execute_input":"2025-10-14T21:17:52.545937Z","iopub.status.idle":"2025-10-14T21:21:17.564303Z","shell.execute_reply.started":"2025-10-14T21:17:52.545910Z","shell.execute_reply":"2025-10-14T21:21:17.563637Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/bert-ner-conll2003/ (stored 0%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/ (stored 0%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/training_args.bin (deflated 52%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/scaler.pt (deflated 60%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/scheduler.pt (deflated 56%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/optimizer.pt (deflated 18%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/config.json (deflated 56%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/trainer_state.json (deflated 73%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-2811/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-ner-conll2003/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/ (stored 0%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/training_args.bin (deflated 52%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/scaler.pt (deflated 60%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/scheduler.pt (deflated 55%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/optimizer.pt (deflated 18%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/config.json (deflated 56%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/trainer_state.json (deflated 71%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-1874/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/ (stored 0%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/training_args.bin (deflated 52%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/scaler.pt (deflated 60%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/rng_state.pth (deflated 25%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/vocab.txt (deflated 49%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/scheduler.pt (deflated 55%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/optimizer.pt (deflated 18%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/config.json (deflated 56%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/trainer_state.json (deflated 67%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-ner-conll2003/checkpoint-937/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/bert-ner-conll2003/model.safetensors (deflated 7%)\n  adding: kaggle/working/bert-ner-conll2003/config.json (deflated 56%)\n  adding: kaggle/working/bert-ner-conll2003/tokenizer.json (deflated 70%)\n  adding: kaggle/working/bert-ner-conll2003/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/bert-ner-conll2003/tokenizer_config.json (deflated 75%)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}